{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg3Qfry5miHbJRKnlA3PLr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chandanlokesh/python-lab/blob/master/mini_proj_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25_URhj3COME",
        "outputId": "5979c625-3b45-4f91-a8cd-810e9a416c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 83ms/step - loss: 3690.6145 - mean_squared_error: 0.2457 - val_loss: 3433.0825 - val_mean_squared_error: 0.2286\n",
            "Epoch 2/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - loss: 3285.4609 - mean_squared_error: 0.2188 - val_loss: 2699.9519 - val_mean_squared_error: 0.1796\n",
            "Epoch 3/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - loss: 2373.3816 - mean_squared_error: 0.1580 - val_loss: 1281.1080 - val_mean_squared_error: 0.0851\n",
            "Epoch 4/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step - loss: 945.5228 - mean_squared_error: 0.0628 - val_loss: 297.3910 - val_mean_squared_error: 0.0194\n",
            "Epoch 5/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - loss: 219.1094 - mean_squared_error: 0.0140 - val_loss: 121.7327 - val_mean_squared_error: 0.0073\n",
            "Epoch 6/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 65ms/step - loss: 101.8400 - mean_squared_error: 0.0059 - val_loss: 75.3484 - val_mean_squared_error: 0.0040\n",
            "Epoch 7/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - loss: 68.1862 - mean_squared_error: 0.0034 - val_loss: 65.2796 - val_mean_squared_error: 0.0033\n",
            "Epoch 8/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step - loss: 58.1755 - mean_squared_error: 0.0026 - val_loss: 45.4221 - val_mean_squared_error: 0.0018\n",
            "Epoch 9/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - loss: 44.3047 - mean_squared_error: 0.0017 - val_loss: 38.5772 - val_mean_squared_error: 0.0013\n",
            "Epoch 10/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 65ms/step - loss: 37.5303 - mean_squared_error: 0.0012 - val_loss: 35.3097 - val_mean_squared_error: 0.0011\n",
            "Epoch 1/10, Generator Loss: 0.4285333454608917, Discriminator Loss: -0.5547817945480347\n",
            "Epoch 2/10, Generator Loss: 1.4108396768569946, Discriminator Loss: -2.079562187194824\n",
            "Epoch 3/10, Generator Loss: 0.6850516200065613, Discriminator Loss: -2.844115734100342\n",
            "Epoch 4/10, Generator Loss: 0.4757104218006134, Discriminator Loss: -4.611058712005615\n",
            "Epoch 5/10, Generator Loss: -4.894842624664307, Discriminator Loss: -0.09731197357177734\n",
            "Epoch 6/10, Generator Loss: 22.799631118774414, Discriminator Loss: -25.6226863861084\n",
            "Epoch 7/10, Generator Loss: 8.05770206451416, Discriminator Loss: -12.258232116699219\n",
            "Epoch 8/10, Generator Loss: 0.5228366851806641, Discriminator Loss: -6.865649700164795\n",
            "Epoch 9/10, Generator Loss: -1.2942497730255127, Discriminator Loss: -8.0923490524292\n",
            "Epoch 10/10, Generator Loss: 11.137468338012695, Discriminator Loss: -23.97945785522461\n",
            "✅ Postprocessed synthetic data saved to 'synthetic_traffic_vae_wgan.csv'.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "# Preprocessing function with get_dummies\n",
        "scaler = None  # Global scaler to maintain consistency\n",
        "\n",
        "def preprocess_csv_with_dummies(file_path):\n",
        "    global scaler\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Detect numerical and categorical columns\n",
        "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    # Normalize numerical columns\n",
        "    if numerical_columns:\n",
        "        scaler = MinMaxScaler()\n",
        "        df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "    # One-hot encode categorical columns using pd.get_dummies\n",
        "    if categorical_columns:\n",
        "        df = pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "    return df.astype(np.float32), numerical_columns, categorical_columns\n",
        "\n",
        "# Load and preprocess data\n",
        "file_path = 'simulated_traffic.csv'\n",
        "processed_data, numerical_columns, categorical_columns = preprocess_csv_with_dummies(file_path)\n",
        "data = processed_data.values\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_temp = train_test_split(data, test_size=0.4, random_state=42)\n",
        "X_valid, X_test = train_test_split(X_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Sampling layer for VAE\n",
        "class Sampling(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# Variational Autoencoder Model\n",
        "class VAE(Model):\n",
        "    def __init__(self, original_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = self.build_encoder(original_dim, latent_dim)\n",
        "        self.decoder = self.build_decoder(original_dim, latent_dim)\n",
        "\n",
        "    def build_encoder(self, original_dim, latent_dim):\n",
        "        inputs = Input(shape=(original_dim,))\n",
        "        x = Dense(128, activation=\"relu\")(inputs)\n",
        "        x = Dense(64, activation=\"relu\")(x)\n",
        "        z_mean = Dense(latent_dim)(x)\n",
        "        z_log_var = Dense(latent_dim)(x)\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "        return Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    def build_decoder(self, original_dim, latent_dim):\n",
        "        latent_inputs = Input(shape=(latent_dim,))\n",
        "        x = Dense(64, activation=\"relu\")(latent_inputs)\n",
        "        x = Dense(128, activation=\"relu\")(x)\n",
        "        outputs = Dense(original_dim, activation=\"sigmoid\")(x)\n",
        "        return Model(latent_inputs, outputs, name=\"decoder\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstructed = self.decoder(z)\n",
        "        reconstruction_loss = tf.keras.losses.mse(inputs, reconstructed)\n",
        "        reconstruction_loss *= tf.cast(tf.shape(inputs)[1], tf.float32)\n",
        "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
        "        self.add_loss(tf.reduce_mean(reconstruction_loss + kl_loss))\n",
        "        return reconstructed\n",
        "\n",
        "# WGAN Generator\n",
        "class Generator(Model):\n",
        "    def __init__(self, data_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.dense1 = Dense(128, activation='relu')\n",
        "        self.dense2 = Dense(256, activation='relu')\n",
        "        self.dense3 = Dense(data_dim, activation='tanh')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return self.dense3(x)\n",
        "\n",
        "# WGAN Discriminator\n",
        "class Discriminator(Model):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.dense1 = Dense(256, activation='relu')\n",
        "        self.dense2 = Dense(128, activation='relu')\n",
        "        self.dense3 = Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return self.dense3(x)\n",
        "\n",
        "# WGAN Training Loop\n",
        "def train_wgan(generator, discriminator, X_train, X_valid, latent_dim, batch_size=64, epochs=10, learning_rate=0.0001): #hfhslhklfhldsklafalsjflas\n",
        "    gen_optimizer = Adam(learning_rate)\n",
        "    disc_optimizer = Adam(learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, X_train.shape[0], batch_size):\n",
        "            real_data = X_train[i:i + batch_size]\n",
        "            batch_size_real = real_data.shape[0]\n",
        "\n",
        "            with tf.GradientTape() as disc_tape:\n",
        "                z = tf.random.normal((batch_size_real, latent_dim))\n",
        "                fake_data = generator(z, training=True)\n",
        "                real_output = discriminator(real_data, training=True)\n",
        "                fake_output = discriminator(fake_data, training=True)\n",
        "                disc_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
        "\n",
        "            grads_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "            disc_optimizer.apply_gradients(zip(grads_disc, discriminator.trainable_variables))\n",
        "\n",
        "            with tf.GradientTape() as gen_tape:\n",
        "                z = tf.random.normal((batch_size_real, latent_dim))\n",
        "                fake_data = generator(z, training=True)\n",
        "                fake_output = discriminator(fake_data, training=True)\n",
        "                gen_loss = -tf.reduce_mean(fake_output)\n",
        "\n",
        "            grads_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "            gen_optimizer.apply_gradients(zip(grads_gen, generator.trainable_variables))\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Generator Loss: {gen_loss.numpy()}, Discriminator Loss: {disc_loss.numpy()}\")\n",
        "\n",
        "# Initialize and train VAE\n",
        "original_dim = data.shape[1]\n",
        "vae_latent_dim = 5\n",
        "vae = VAE(original_dim, vae_latent_dim)\n",
        "vae.compile(optimizer=Adam(learning_rate=0.0001), metrics=[MeanSquaredError()])\n",
        "vae.fit(X_train, X_train, validation_data=(X_valid, X_valid), epochs=10, batch_size=64) #798798798790898njsaklhjfklah\n",
        "\n",
        "# Train WGAN\n",
        "latent_dim = 10\n",
        "generator = Generator(original_dim)\n",
        "discriminator = Discriminator()\n",
        "train_wgan(generator, discriminator, X_train, X_valid, latent_dim)\n",
        "\n",
        "# Generate synthetic data\n",
        "def generate_synthetic_data(generator, num_samples, latent_dim):\n",
        "    z = tf.random.normal((num_samples, latent_dim))\n",
        "    synthetic_data = generator(z, training=False).numpy()\n",
        "    return synthetic_data\n",
        "\n",
        "synthetic_samples = generate_synthetic_data(generator, num_samples=1000, latent_dim=latent_dim)\n",
        "\n",
        "def postprocess_synthetic_data(synthetic_data, original_df, numerical_columns, categorical_columns):\n",
        "    global scaler\n",
        "    df = pd.DataFrame(synthetic_data, columns=original_df.columns)\n",
        "\n",
        "    # columns_to_convert = [\"Packet Count\", \"Byte Count\", \"Flow Duration (ms)\", \"Idle Time (ms)\", \"Active Time (ms)\"]\n",
        "\n",
        "    # for column in columns_to_convert:\n",
        "    #     if column in numerical_columns:\n",
        "    #         df[column] = df[column].round().astype(int)\n",
        "\n",
        "    # Denormalize numerical columns\n",
        "    if numerical_columns and scaler:\n",
        "        df[numerical_columns] = scaler.inverse_transform(df[numerical_columns])\n",
        "\n",
        "        # Clip and round ports to valid range\n",
        "        if 'src_port' in numerical_columns:\n",
        "            df['src_port'] = df['src_port'].clip(0, 65535).round().astype(int)\n",
        "        if 'dst_port' in numerical_columns:\n",
        "            df['dst_port'] = df['dst_port'].clip(0, 65535).round().astype(int)\n",
        "\n",
        "\n",
        "        # if \"Packet Count\" in numerical_columns:\n",
        "        #     df['Packet Count'] = df['Packet Count'].round().astype(int)\n",
        "        # if \"Byte Count\" in numerical_columns:\n",
        "        #     df['Byte Count'] = df['Byte Count'].round().astype(int)\n",
        "        # if \"Flow Duration (ms)\" in numerical_columns:\n",
        "        #     df['Flow Duration (ms)'] = df['Flow Duration (ms)'].round().astype(int)\n",
        "        # if \"Idle Time (ms)\" in numerical_columns:\n",
        "        #     df['Idle Time (ms)'] = df['Idle Time (ms)'].round().astype(int)\n",
        "\n",
        "        # if \"Active Time (ms)\" in numerical_columns:\n",
        "        #     df['Active Time (ms)'] = df['Active Time (ms)'].round().astype(int)\n",
        "\n",
        "\n",
        "    # Convert one-hot encoded columns back to original categories\n",
        "    for cat_col in categorical_columns:\n",
        "        cat_prefix = [col for col in df.columns if col.startswith(cat_col + '_')]\n",
        "        if cat_prefix:\n",
        "            df[cat_col] = df[cat_prefix].idxmax(axis=1).apply(lambda x: x.split('_', 1)[-1])\n",
        "            df = df.drop(columns=cat_prefix)\n",
        "\n",
        "\n",
        "    if 'Attack Type' in df.columns:\n",
        "        df['Label'] = df['Attack Type'].apply(lambda x: 'Benign' if x.lower() in ['normal', 'benign'] else 'Malicious')\n",
        "\n",
        "\n",
        "    df['src_port'] = df['src_port'].fillna(0).astype(int)\n",
        "    df['dst_port'] = df['dst_port'].fillna(0).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "synthetic_data_df = postprocess_synthetic_data(synthetic_samples, processed_data, numerical_columns, categorical_columns)\n",
        "\n",
        "# Save final CSV\n",
        "synthetic_data_df.to_csv('synthetic_traffic_vae_wgan.csv', index=False)\n",
        "print(\"✅ Postprocessed synthetic data saved to 'synthetic_traffic_vae_wgan.csv'.\")\n"
      ]
    }
  ]
}